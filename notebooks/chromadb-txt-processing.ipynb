{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw4LNOXlpCQZ"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLJm1U9gpHou"
      },
      "source": [
        "# Set up ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"../db/chroma\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "collection_name = \"text_documents\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete collection if it exists (optional)\n",
        "try:\n",
        "    client.delete_collection(name=collection_name)\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "collection = client.create_collection(\n",
        "    name=collection_name, \n",
        "    metadata={\n",
        "        \"description\": \"text documents collection\",\n",
        "        \"created\": str(datetime.now())\n",
        "    }  \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set up Text Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import uuid\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the tokenizer for token counting\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Using OpenAI's tokenizer for approximation\n",
        "\n",
        "def count_tokens(text):\n",
        "    \"\"\"Count the number of tokens in a text string\"\"\"\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "def split_text_by_tokens(text, max_tokens=300, overlap_tokens=100):\n",
        "    \"\"\"Split text into chunks with a maximum token count and optional overlap\"\"\"\n",
        "    # First, tokenize the entire text\n",
        "    tokens = tokenizer.encode(text)\n",
        "    \n",
        "    chunks = []\n",
        "    chunk_start = 0\n",
        "    \n",
        "    while chunk_start < len(tokens):\n",
        "        # Get chunk end position\n",
        "        chunk_end = min(chunk_start + max_tokens, len(tokens))\n",
        "        \n",
        "        # Get the tokens for this chunk and decode back to text\n",
        "        chunk_tokens = tokens[chunk_start:chunk_end]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        \n",
        "        # Add the chunk to our list\n",
        "        chunks.append(chunk_text)\n",
        "        \n",
        "        # Move to the next position with overlap consideration\n",
        "        chunk_start += max_tokens - overlap_tokens\n",
        "        # Avoid getting stuck in an infinite loop\n",
        "        if chunk_start >= chunk_end:\n",
        "            chunk_start = chunk_end\n",
        "            \n",
        "    return chunks\n",
        "\n",
        "def load_txt_file(file_path):\n",
        "    \"\"\"Load content from a text file\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
        "        return file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "embedding_model = \"nomic-embed-text\"\n",
        "\n",
        "embeddings = OllamaEmbeddings(\n",
        "    model=embedding_model,\n",
        ")\n",
        "\n",
        "def embedding_generation(text):\n",
        "    \"\"\"Generate embedding for a text chunk\"\"\"\n",
        "    return embeddings.embed_query(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Process Text Files and Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_text_files(directory_path):\n",
        "    \"\"\"Process all text files in a directory, split into chunks, and generate embeddings\"\"\"\n",
        "    # Get all .txt files in the directory\n",
        "    txt_files = glob.glob(os.path.join(directory_path, \"*.txt\"))\n",
        "    \n",
        "    all_chunks = []\n",
        "    \n",
        "    # Process each file\n",
        "    for file_path in txt_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        print(f\"Processing {file_name}\")\n",
        "        \n",
        "        # Load the text content\n",
        "        text_content = load_txt_file(file_path)\n",
        "        \n",
        "        # Split into chunks\n",
        "        chunks = split_text_by_tokens(text_content)\n",
        "        print(f\"  Split into {len(chunks)} chunks\")\n",
        "        \n",
        "        # Process each chunk\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = str(uuid.uuid4())\n",
        "            embedding = embedding_generation(chunk)\n",
        "            \n",
        "            chunk_data = {\n",
        "                \"id\": chunk_id,\n",
        "                \"source_file\": file_name,\n",
        "                \"chunk_index\": i,\n",
        "                \"text\": chunk,\n",
        "                \"token_count\": count_tokens(chunk),\n",
        "                \"embedding\": embedding\n",
        "            }\n",
        "            \n",
        "            all_chunks.append(chunk_data)\n",
        "    \n",
        "    print(f\"Processed a total of {len(all_chunks)} chunks from {len(txt_files)} files\")\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_chunks_to_collection(chunks, collection):\n",
        "    \"\"\"Add processed chunks to the ChromaDB collection\"\"\"\n",
        "    # Add chunks in batches to avoid potential issues with large datasets\n",
        "    batch_size = 100\n",
        "    \n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        \n",
        "        # Extract the data needed for the collection\n",
        "        ids = [chunk[\"id\"] for chunk in batch]\n",
        "        documents = [chunk[\"text\"] for chunk in batch]\n",
        "        embeddings_list = [chunk[\"embedding\"] for chunk in batch]\n",
        "        metadatas = [\n",
        "            {\n",
        "                \"source_file\": chunk[\"source_file\"],\n",
        "                \"chunk_index\": chunk[\"chunk_index\"],\n",
        "                \"token_count\": chunk[\"token_count\"]\n",
        "            } for chunk in batch\n",
        "        ]\n",
        "        \n",
        "        # Add to collection\n",
        "        collection.add(\n",
        "            ids=ids,\n",
        "            documents=documents,\n",
        "            embeddings=embeddings_list,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "        \n",
        "        print(f\"Added batch {i//batch_size + 1} to collection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed chunks to a file\n",
        "import json\n",
        "\n",
        "def save_processed_chunks(chunks, output_path):\n",
        "    \"\"\"Save processed chunks to a JSON file\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    \n",
        "    # Save the data\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(chunks, file, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"Saved processed chunks to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Directory containing text files\n",
        "txt_directory = \"../data/txt_files\"\n",
        "output_file = \"../embeddings/text_embeddings.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 180824_Medellin_105x190_ES_ConvBureau.txt\n",
            "  Split into 58 chunks\n",
            "Processing Medelln.txt\n",
            "  Split into 13 chunks\n",
            "Processed a total of 71 chunks from 2 files\n"
          ]
        }
      ],
      "source": [
        "# Process all text files\n",
        "processed_chunks = process_text_files(txt_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved processed chunks to ../embeddings/text_embeddings.json\n"
          ]
        }
      ],
      "source": [
        "# Save processed chunks\n",
        "save_processed_chunks(processed_chunks, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added batch 1 to collection\n"
          ]
        }
      ],
      "source": [
        "# Add chunks to ChromaDB collection\n",
        "add_chunks_to_collection(processed_chunks, collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents in collection: 71\n"
          ]
        }
      ],
      "source": [
        "# Verify the number of documents in the collection\n",
        "print(f\"Total documents in collection: {collection.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_similar_chunks(query_text, n_results=10):\n",
        "    \"\"\"Query the collection for chunks similar to the query text\"\"\"\n",
        "    query_embedding = embedding_generation(query_text)\n",
        "    \n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n_results,\n",
        "    )\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Result 1 (similarity: 0.4674)\n",
            "Source: Medelln.txt, Chunk: 0\n",
            "Preview: Guía de Medellín\n",
            "\n",
            "Medellín, ubicada dentro del departamento de Antioquia, es una ciudad dinámica que ha sufrido\n",
            "una transformación cultural, social y económica en los últimos años.\n",
            "Fundada hace más de...\n",
            "\n",
            "Result 2 (similarity: 0.4448)\n",
            "Source: 180824_Medellin_105x190_ES_ConvBureau.txt, Chunk: 5\n",
            "Preview:  desconectado.\n",
            "\n",
            "2\n",
            "Introducción\n",
            "\n",
            "7\n",
            "\n",
            "\fGreater Medellín Convention\n",
            "and Visitors Bureau\n",
            "“La Ciudad más Innovadora del Mundo” en 2013 y el “Mejor Destino\n",
            "Turístico de Sudamérica” en 2018, son algunos de lo...\n",
            "\n",
            "Result 3 (similarity: 0.4030)\n",
            "Source: 180824_Medellin_105x190_ES_ConvBureau.txt, Chunk: 8\n",
            "Preview: , la Catedral\n",
            "es el principal templo de la Arquidiócesis de Medellín, declarada\n",
            "Monumento Nacional de Colombia\n",
            "el 12 de marzo de 1982. Se dice\n",
            "que es la iglesia más grande del\n",
            "mundo construida en ladr...\n",
            "\n",
            "Result 4 (similarity: 0.4000)\n",
            "Source: 180824_Medellin_105x190_ES_ConvBureau.txt, Chunk: 31\n",
            "Preview: acido en Medellín,\n",
            "así como disfrutar de un mercado\n",
            "de artesanías, llamado Sanalejo,\n",
            "el primer sábado de cada mes.\n",
            "Sitio tradicional de Medellín para\n",
            "el disfrute ciudadano, rodeado de\n",
            "cultura, ideal p...\n",
            "\n",
            "Result 5 (similarity: 0.3932)\n",
            "Source: Medelln.txt, Chunk: 5\n",
            "Preview:  Botero\n",
            "Medellín, la ciudad que vio nacer al gran maestro Fernando Botero, tiene la colección más grande\n",
            "del artista con casi 200 obras exhibidas en diferentes lugares de la ciudad como el Parque San\n",
            "...\n",
            "\n",
            "Result 6 (similarity: 0.3921)\n",
            "Source: Medelln.txt, Chunk: 1\n",
            "Preview:  urbano alberga eventos de talla mundial como el Festival de las Flores, el Festival\n",
            "Internacional de Poesía, el Festival del Libro y la Cultura de Medellín, Colombiamoda y Colombiatex.\n",
            "Medellín ha si...\n",
            "\n",
            "Result 7 (similarity: 0.3794)\n",
            "Source: 180824_Medellin_105x190_ES_ConvBureau.txt, Chunk: 10\n",
            "Preview: \n",
            "urbano de Medellín. Se dice que\n",
            "templo religioso de culto católico\n",
            "una de sus campanas le sirvió a\n",
            "bajo la advocación de la Vera Cruz o Francisco José de Caldas “El SaSanta Cruz. Está situada en el c...\n",
            "\n",
            "Result 8 (similarity: 0.3743)\n",
            "Source: Medelln.txt, Chunk: 7\n",
            "Preview: . Además, en la ciudad se puede apreciar el patrimonio cultural representado en verdaderas\n",
            "\n",
            "\fjoyas arquitectónicas y religiosas como la Catedral Basílica Metropolitana y la Basílica Menor\n",
            "Nuestra Seño...\n",
            "\n",
            "Result 9 (similarity: 0.3724)\n",
            "Source: 180824_Medellin_105x190_ES_ConvBureau.txt, Chunk: 9\n",
            "Preview:  siglos XVIII y XIX).\n",
            "\n",
            "Basílica Menor Nuestra Señora de la Candelaria\n",
            "Ubicada en el centro de Medellín,\n",
            "es la primera parroquia de la\n",
            "ciudad. El Templo de culto católico\n",
            "se dedicó a Nuestra Señora\n",
            "de ...\n",
            "\n",
            "Result 10 (similarity: 0.3687)\n",
            "Source: Medelln.txt, Chunk: 11\n",
            "Preview: ales para comprar productos artesanales únicos como la\n",
            "cerámica de El Carmen de Viboral y las guitarras de Marinilla.\n",
            "Occidente (55 km desde Medellín)\n",
            "A poco más de una hora de Medellín, se puede disf...\n"
          ]
        }
      ],
      "source": [
        "# Test query - replace with your own query\n",
        "test_query = \"Api\"\n",
        "results = query_similar_chunks(test_query)\n",
        "\n",
        "# Display results\n",
        "for i, (doc, metadata, distance) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):\n",
        "    print(f\"\\nResult {i+1} (similarity: {1-distance:.4f})\")\n",
        "    print(f\"Source: {metadata['source_file']}, Chunk: {metadata['chunk_index']}\")\n",
        "    print(f\"Preview: {doc[:200]}...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Zw4LNOXlpCQZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
